{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MO2dFZnX99Th"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# CIFAR-10 dataset with normalization and transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training data\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=0)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "US6pqmODFD7v",
    "outputId": "738cf0e9-f813-44af-b017-04479e92d0c9"
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # 输入是 Z, 对Z进行卷积\n",
    "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            # 上采样至 8x8\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            # 上采样至 16x16\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # 上采样至 32x32\n",
    "            nn.ConvTranspose2d(128, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # 输出图像的尺寸是 3x32x32\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = z.view(-1, 100, 1, 1)\n",
    "        return self.model(z)\n",
    "# Critic (Discriminator)\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # 输入图像尺寸是 3x32x32\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 下采样至 16x16\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 下采样至 8x8\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 下采样至 4x4\n",
    "            nn.Conv2d(256, 1, 4, 1, 0, bias=False),\n",
    "            # 输出一个单一的数值\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "x7FpJkzv-Snm"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move models to device\n",
    "NetG = Generator().to(device)\n",
    "NetC = Critic().to(device)"
   ],
   "metadata": {
    "id": "L8BUgKr4Vqay"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Wasserstein Loss Function\n",
    "def wasserstein_loss(y_real, y_fake):\n",
    "    return torch.mean(y_fake) - torch.mean(y_real)\n",
    "\n",
    "# Gradient Penalty function\n",
    "def gradient_penalty(critic, real_data, fake_data):\n",
    "    # Ensure tensors are on the same device\n",
    "    alpha = torch.rand(real_data.size(0), 1, 1, 1).to(real_data.device)\n",
    "    interpolates = alpha * real_data + (1 - alpha) * fake_data\n",
    "\n",
    "    interpolates.requires_grad_(True)\n",
    "    critic_interpolates = critic(interpolates)\n",
    "\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=critic_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones_like(critic_interpolates),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "\n",
    "    gradient_norm = gradients.view(gradients.size(0), -1).norm(2, dim=1)\n",
    "    penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "\n",
    "    return penalty"
   ],
   "metadata": {
    "id": "txwW0-CtHE7r"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Function to compute the gradient norm\n",
    "def gradient_norm(model):\n",
    "    norms = []\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            norms.append(param.grad.norm().item())\n",
    "    return norms\n",
    "\n",
    "# Train the WGAN, capturing losses, gradient norms, and printing updates\n",
    "def train_wgan(generator, critic, dataloader, num_epochs, device):\n",
    "    optimizer_G = optim.RMSprop(generator.parameters(), lr=5e-5)\n",
    "    optimizer_D = optim.RMSprop(critic.parameters(), lr=5e-5)\n",
    "\n",
    "    # Lists to store generator and critic losses, and gradient norms\n",
    "    generator_losses = []\n",
    "    critic_losses = []\n",
    "    gradient_norms_generator = []\n",
    "    gradient_norms_critic = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_generator_loss = 0\n",
    "        epoch_critic_loss = 0\n",
    "        generator_norms = []\n",
    "        critic_norms = []\n",
    "\n",
    "        iterations = 0\n",
    "        for i, (data, _) in enumerate(dataloader):\n",
    "            optimizer_D.zero_grad()\n",
    "            real_data = data.to(device)\n",
    "            z = torch.randn(real_data.size(0), 100).to(device)\n",
    "            fake_data = generator(z)\n",
    "\n",
    "            # Critic update\n",
    "            critic_loss = wasserstein_loss(critic(real_data), critic(fake_data)) + gradient_penalty(critic, real_data, fake_data)\n",
    "            critic_loss.backward()\n",
    "            # critic_norms.append(gradient_norm(critic))  # Store gradient norms   ###\n",
    "            optimizer_D.step()\n",
    "\n",
    "            epoch_critic_loss += critic_loss.item()\n",
    "            iterations += 1\n",
    "\n",
    "            # Generator update every 5 iterations\n",
    "            if i % 1 == 0:\n",
    "                optimizer_G.zero_grad()\n",
    "                # z = torch.randn(real_data.size(0), 100).to(device)  ###\n",
    "                fake_data = generator(z)\n",
    "                generator_loss = -torch.mean(critic(fake_data))\n",
    "                generator_loss.backward()\n",
    "                # generator_norms.append(gradient_norm(generator))  # Store gradient norms  ###\n",
    "                optimizer_G.step()\n",
    "\n",
    "                epoch_generator_loss += generator_loss.item()\n",
    "\n",
    "        # Store the average losses and gradient norms\n",
    "        generator_losses.append(epoch_generator_loss / (iterations / 5))\n",
    "        critic_losses.append(epoch_critic_loss / iterations)\n",
    "        gradient_norms_generator.append(generator_norms)\n",
    "        gradient_norms_critic.append(critic_norms)   ###\n",
    "\n",
    "        # Print epoch progress\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Generator Loss: {generator_losses[-1]:.4f}, Critic Loss: {critic_losses[-1]:.4f}')\n",
    "\n",
    "    return generator_losses, critic_losses, gradient_norms_critic, gradient_norms_generator\n"
   ],
   "metadata": {
    "id": "WWlQW5Ud-vkP"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "generator_losses, critic_losses, gradient_norms_critic, gradient_norms_generator = train_wgan(NetG, NetC, trainloader, num_epochs=10,device = device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uk24t_p1cwOv",
    "outputId": "68df70a5-7bda-4fba-dc17-f3a0dd58d25d",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Generator Loss: 38.1202, Critic Loss: -3.0958\n",
      "Epoch 2/10, Generator Loss: 50.9144, Critic Loss: -1.6543\n",
      "Epoch 3/10, Generator Loss: 68.3295, Critic Loss: -1.9502\n",
      "Epoch 4/10, Generator Loss: 71.4447, Critic Loss: -1.5505\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epochs = list(range(1, 11))\n",
    "\n",
    "plt.plot(epochs, generator_losses, label='Generator Loss')\n",
    "plt.plot(epochs, critic_losses, label='Critic Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Generator and Critic Losses over 20 Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "P0Pr54jKkbq0",
    "outputId": "5e78e05d-b94b-4212-ce20-ea11c2620c1f",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(len(gradient_norms_generator[0])):\n",
    "    norms = [gradient_norms_generator[j][i] for j in range(len(gradient_norms_generator))]\n",
    "    plt.plot(epochs, norms, label=f'Parameter {i+1}')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Gradient Norm')\n",
    "plt.title('Gradient Norms over Epochs')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "_mcun5byla_N",
    "outputId": "3f86d3fd-3740-47f3-d381-df9136ca2516",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to generate and display images\n",
    "def generate_images(generator, num_images=24, noise_dim=100, grid_size=(8, 8)):\n",
    "    # Create random noise for image generation\n",
    "    noise = torch.randn(num_images, noise_dim).to(generator.model[0].weight.device)\n",
    "\n",
    "    # Generate images from the noise\n",
    "    with torch.no_grad():  # No gradients needed during inference\n",
    "        generated_images = generator(noise)\n",
    "\n",
    "    # Create a grid of images for visualization\n",
    "    image_grid = vutils.make_grid(generated_images, nrow=grid_size[0], padding=2, normalize=True)\n",
    "    image_grid = image_grid.cpu()\n",
    "\n",
    "    # Display the image grid\n",
    "    plt.imshow(image_grid.permute(1, 2, 0))  # Convert to HxWxC format\n",
    "    plt.axis('off')  # Hide axes\n",
    "    plt.show()  # Display the generated images\n",
    "    return generated_images\n"
   ],
   "metadata": {
    "id": "CuKHK_iHOYBX",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "fake_images = generate_images(NetG, 20)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "7yk3a3HFOa3_",
    "outputId": "0e6a06e2-e19a-41aa-fefa-b730d3c8cdc4",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "# 假定 fake_images, n_classes, n_examples 已经定义\n",
    "def show_images(images, num_images, nrow=10, names=None, name_scale=1):\n",
    "    plt.figure(figsize=(15 / 10 * nrow, 15))\n",
    "    images = make_grid(images.cpu()[:num_images], nrow=nrow, padding=2, pad_value=1)\n",
    "    images =  np.clip(images, 0, 1)\n",
    "    npimg = images.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    single_img_height = npimg.shape[1] // (num_images // nrow) * name_scale\n",
    "    if names is not None:\n",
    "        for i, name in enumerate(names):\n",
    "            if i >= num_images:\n",
    "                break  # Break if there are more names than images\n",
    "            row = i // nrow - 1\n",
    "            col = i % nrow\n",
    "            plt.text(col * single_img_height + single_img_height / 2, (row + 1) * single_img_height - single_img_height * 0.14,\n",
    "                     name, ha='center', va='top', fontsize=12, color='black',\n",
    "                     bbox=dict(facecolor='white', alpha=0.5, edgecolor='white', boxstyle='round,pad=0.1'))\n",
    "    plt.show()\n",
    "show_images(fake_images/ 2 + 0.5, 20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ]
}
